<script>
	import ProjectLinkButton from '$lib/components/ProjectLinkButton.svelte';
	import TechnologyPills from '$lib/components/TechnologyPills.svelte';
	import Img from '@zerodevx/svelte-img';
	import twoButtonsMeme from "$lib/images/twobuttonsmeme.jpg?as=run";
	import { browser } from '$app/environment';
	import { onMount, onDestroy } from 'svelte';
	import Card from '$lib/components/Card.svelte';

	const {
		data
	} = $props();

	const totalCountries = $derived(data.totalCountries);
	const dataPointsPerYearChart = $derived(data.dataPointsPerYearChart);
	const totalIndicators = $derived(data.totalIndicators);
	const error = $derived(data.error);

	let apexChartInstance = $state(null);
	let chartElementId = 'wdi-data-points-chart'; // Unique ID for the chart element

	onMount(async () => {
		if (browser) {
			const module = await import('apexcharts');
			const ApexCharts = module.default;

			if (dataPointsPerYearChart?.series?.[0]?.data?.length > 0) {
				const chartEl = document.getElementById(chartElementId);
				if (chartEl) {
					const options = {
						series: dataPointsPerYearChart.series,
						chart: {
							type: 'bar',
							height: 350,
							toolbar: { show: true },
							foreColor: '#A0AEC0' /* gray-500 for axis text */
						},
						plotOptions: {
							bar: {
								horizontal: false,
								columnWidth: '55%',
								endingShape: 'rounded'
							}
						},
						colors: ['#166534'], // Set bar color to Tailwind green-800 (hex for #16a34a is green-600, #166534 is green-800)
						dataLabels: { enabled: false },
						xaxis: { type: 'category', title: { text: 'Year', style: { color: '#A0AEC0' } } },
						yaxis: { title: { text: 'Number of Data Points', style: { color: '#A0AEC0' } } },
						theme: { mode: 'dark' }
					};
					apexChartInstance = new ApexCharts(chartEl, options);
					await apexChartInstance.render();
				}
			}
		}
	});

	onDestroy(() => {
		if (apexChartInstance) {
			apexChartInstance.destroy();
		}
	});

	const projectUrl = "https://github.com/nickbrett1/dbt-duckdb";
	const techs = ["dbt", "DuckDB", "Cloudflare"];

	import { GithubBrands } from 'svelte-awesome-icons';

</script>

<style>
	.responsive-text-table :global(pre.shiki) {
		white-space: pre-wrap; /* Preserves whitespace, but wraps lines */
		overflow-wrap: break-word; /* Breaks long words to prevent overflow */
		word-break: break-all; /* More aggressive word breaking if needed */
		overflow-x: hidden; /* Explicitly hide horizontal scrollbar if wrapping is effective */
	}
	.mermaid-container {
		display: flex;
		justify-content: center;
	}
</style>

<h1 class="!mb-0 no-underline">Modern ETL without a Data Warehouse</h1>
<p class="text-xl text-gray-300 !mt-2 !mb-6">A dbt and DuckDB approach</p>

<ProjectLinkButton href={projectUrl} />

## Introduction

Data transformation is a critical step in any data pipeline. For many use cases,[^1] (www.x.com) (~~stuff~~) a full-scale cloud data warehouse offers powerful solutions, providing scalability for large datasets and easy access to analytical tools. However, this power often comes with increased complexity, cost, and maintenance overhead. At the other end of the spectrum, a few simple scripts working on a data dump offers an easy starting point but lacks the robust structure needed for managing complex transformations as projects evolve.

<!-- Use https://imgflip.com/ for memes -->
<Img src={twoButtonsMeme} alt="Meme illustrating the choice between simple scripts and complex cloud data warehouses" class="mx-auto my-4 max-w-md" />

This article describes a middle ground between these two approaches, aimed at scenarios where datasets are manageable in size, somewhat independent, for example for driving trading decisions, and yet there's a strong need for well-defined, maintainable transformations without incurring the costs and complexities of a full-scale data warehouse.

We'll demonstrate how to build such a pipeline and deploy with minimal infrastructure by leveraging the combined strengths of [dbt-core](https://github.com/dbt-labs/dbt-core) and [DuckDB](https://duckdb.org/). This approach offers reduced cloud dependencies for the core transformation engine, lower costs, and a simplified setup and management process. As a practical example, we'll use the [World Bank World Development Indicators](https://databank.worldbank.org/source/world-development-indicators), showing code snippets and illustrating how robust results can be derived from a relatively simple local data stack.

## Goals

Our specific goals for this project were:

1.  **Integrated Data Model:** Our primary data source was the [WDI CSV download](https://databank.worldbank.org/data/download/WDI_CSV.zip), but we also needed to enrich this by integrating data from the [World Bank's REST API](https://datahelpdesk.worldbank.org/knowledgebase/articles/898590-api-overview). A key requirement was to unify these sources into a consistent data model with standardized naming conventions.
2.  **Data Reshaping for Analysis:** The raw WDI data often presents historical figures in a wide, column-oriented format. To facilitate easier querying and time-series analysis, we aimed to reshape this into a long, row-oriented format.

For example:

<div class="responsive-text-table">

<!-- svelte-ignore a11y_no_noninteractive_tabindex -->

```text
// Wide Format (Column-Oriented)
// Each row represents one country & indicator, with years as columns.

Country Name | Indicator Name | 1960 | 1961 | ...
-------------|----------------|------|------|----
Aruba        | GDP per capita | 100  | 102  | ...
Afghanistan  | Population     | 5000 | 5050 | ...
```

</div>
<div class="responsive-text-table">
<!-- svelte-ignore a11y_no_noninteractive_tabindex -->

```text
// Long Format (Row-Oriented)
// Each row represents one country, indicator, and year observation.

Country Name | Indicator Name | Year | Value
-------------|----------------|------|-------
Aruba        | GDP per capita | 1960 | 100
Aruba        | GDP per capita | 1961 | 102
Afghanistan  | Population     | 1960 | 5000
Afghanistan  | Population     | 1961 | 5050
...          | ...            | ...  | ...
```

</div>

3.  **Clear Data Layering:** We emphasized a clear architectural separation between raw data, an intermediate staging layer, and the final marts layer.

        - **Raw Data Layer:** This is the initial landing zone for data as it's ingested from various sources (e.g., CSV files, API extracts). The data here is typically in its original, unaltered format, reflecting the structure and content provided by the source systems.
        - **Staging Layer:** This layer takes the raw, ingested data and applies initial transformations, such as standardizing column names (e.g., converting "Country Name" to `country_name`), casting data types, and basic cleaning. This ensures a consistent foundation for subsequent transformations.
        - **Marts Layer:** This layer builds upon the staging layer to create polished, consumer-ready datasets, often aggregated or reshaped for specific analytical purposes.

        This separation is crucial for maintainability, allowing us to evolve ingestion or intermediate transformation logic without immediately impacting downstream users. The diagram below illustrates this flow:

<!-- svelte-ignore a11y_no_noninteractive_tabindex -->
<div class="mermaid-container">

```mermaid
graph TD;
		A["Raw Data Layer&#10(Original Format)"] --> B["Staging Layer&#10(Standardized & Cleaned)"];
		B --> C["Marts Layer&#10(Aggregated & Reshaped)"];
		C --> D["Analytics & Applications"];

		click A "https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/staging/_wdi_sources.yml" "View Raw Data Layer dbt sources" _self
		click B "https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/staging/_wdi_raw_models.yml" "View Staging Layer dbt models" _self
		click C "https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/marts/_wdi_marts.yml" "View Marts Layer dbt models" _self

		classDef default fill:#222,stroke:#cccccc,stroke-width:2px,color:#faf0e6;
		linkStyle default stroke:#cccccc,stroke-width:2px;

```

</div>
        <!-- svelte-ignore a11y_no_noninteractive_tabindex -->

4.  **Pre-calculated Aggregations:** To optimize query performance and reduce computational load for common analytical questions, we wanted to be able to pre-calculate frequently used aggregations.
5.  **Comprehensive Documentation and Lineage:** We wanted a way to provide documentation on the transformation, a clean way to provide definitions for the data interface, and transparent lineage explaining how each data point was derived from its original source.
6.  **Flexible and Cost-Effective Output:** To ensure flexibility for any downstream consumers, our strategy was to primarily deliver final datasets as Parquet files in a cloud object store ([Cloudflare R2](https://www.cloudflare.com/developer-platform/r2/) in our case, chosen for its cost-effectiveness and lack of egress fees). We also aimed to demonstrate the capability to easily load this transformed data into a relational database (e.g., [Cloudflare's D1](https://www.cloudflare.com/developer-platform/d1/) serverless SQLite).
7.  **Low-Complexity and Cost-Effective Solution:** We also wanted a solution that was easy to maintain and was relatively low cost given the scale of the project.

## Why Standard Solutions Can Fall Short

Let's compare the two standard solutions: simple scripting versus a full cloud data warehouse and supporting tools.

| Goal                                         | Simple Scripting & File-based                                                                   | Cloud Data Warehouse                                                                                                                |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| **Integrated Data Model**                    | 🔴 Difficult to manage consistency as complexity grows. Ad-hoc.                                 | 🟢 Strong. Designed for structured, relational data models.                                                                         |
| **Data Reshaping for Analysis**              | 🟡 Possible, but complex reshaping (e.g., wide to long) can become unwieldy and error-prone.    | 🟢 Excellent. SQL provides powerful reshaping capabilities.                                                                         |
| **Clear Data Layering**                      | 🔴 Lacks inherent structure for layering; becomes a tangled web of scripts.                     | 🟢 Good. Supports logical separation (e.g., staging, marts).                                                                        |
| **Pre-calculated Aggregations**              | 🟡 Can be done, but managing dependencies and updates is manual and complex.                    | 🟢 Strong. Materialized views or summary tables are common.                                                                         |
| **Comprehensive Documentation & Lineage**    | 🔴 Almost entirely manual and often neglected. Difficult to trace data flow.                    | 🟢 Good. Many tools offer automated documentation and lineage.                                                                      |
| **Flexible & Cost-Effective Output**         | 🟡 Flexible in output formats (files). Cost-effective for simple outputs.                       | 🟡 Flexible (SQL access, connectors), but storage/egress can be costly.                                                             |
| **Low-Complexity & Cost-Effective Solution** | 🟡 Low initial complexity & cost. Becomes very complex and costly to maintain as project grows. | 🔴 High initial setup complexity & potentially high ongoing costs. Managed services reduce some operational burden but add to cost. |

As the table illustrates, neither extreme perfectly aligns with our project's needs:

### The Cloud Data Warehouse Approach: When Comprehensive Becomes Complex

For projects like ours, opting for a full **Cloud Data Warehouse (CDW)** solution (e.g., Snowflake, Google BigQuery, Amazon Redshift) presents a significant hurdle: the inherent **complexity and overhead of its comprehensive ecosystem**. While CDWs excel at structured transformations, lineage, and scaling for very large datasets—often integrating with a rich array of tools for ingestion, orchestration, and BI—this power comes at a cost. Setting up, configuring, and managing these interconnected services, even with provider-managed infrastructure, introduces a layer of operational complexity and potential expense that can be disproportionate for smaller to medium-scale data transformation tasks like our WDI project. Furthermore, committing to a specific CDW often involves a degree of vendor lock-in, which can have long-term cost and flexibility implications. This was an overarching problem we aimed to avoid.

### The Simple Scripting Approach: Trading Robustness for Initial Simplicity

Conversely, while a **Simple Scripting & File-based** approach (e.g., using Python or Shell scripts) offers low initial cost and setup effort, it typically sacrifices long-term **robustness, maintainability, and scalability**. The free-form nature of scripts makes it hard to reason about the transformations occurring, especially as the number of data sources, transformation steps, and interdependencies grows—as was the case with our project's multiple goals. This lack of inherent structure leads to difficulties in testing, debugging, and evolving the pipeline, particularly in a team environment.

## Seeking a Better Way

Given these challenges, the central question becomes:

How can we implement a data transformation workflow that is:

1. Well-structured and easy to evolve?
2. Low-cost and simple to deploy and manage?
3. Capable of efficiently handling typical analytical transformations?

## The Solution: dbt-core Meets DuckDB

The solution we use lies in the powerful combination of **dbt-core** and **DuckDB**.

### Introducing dbt (Data Build Tool)

dbt is an open-source command-line tool that empowers data analysts and engineers to transform data within their data store more effectively. It brings software engineering best practices to the analytics workflow.

Its core capabilities include:

- **SQL-centric (and Python):** Define transformations primarily using SQL (with Python model support in newer versions), a language familiar to most data practitioners.
  For example, one aggregation transformation from our project looks like this:
    <!-- svelte-ignore a11y_no_noninteractive_tabindex -->

  ```sql
  select year, count(value) as data_points_count
  from {{ ref('fct_wdi_history') }}
  where value is not null
  group by year
  ```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/marts/agg_wdi_data_points_by_year.sql" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

- **Dependency Management:** dbt automatically understands and manages the dependencies between transformation steps (called "models").
- **Automated Testing:** Implement data quality tests to ensure the integrity of your transformations and output data.
  For example, we can test for nulls by providing yaml configuration to dbt:
  <!-- svelte-ignore a11y_no_noninteractive_tabindex -->

  ```yaml
  - name: indicator_code
  description: "Indicator code."
  tests:
  	- not_null
  ```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/marts/_wdi_marts.yml" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

- **Documentation & Lineage:** Automatically generate comprehensive documentation and a visual lineage graph, showing how data flows through your pipeline. See [here](./dbt-duckdb/docs) for the generated documentation for the project.
- **Version Control:** Treats your transformation logic as code, enabling version control.

For this project, we used [dbt-core](https://github.com/dbt-labs/dbt-core), the open-source Python package that you can run anywhere. There's also [dbt Cloud](https://www.getdbt.com/), a managed cloud-based solution offering additional features like a scheduler and UI, but `dbt-core` provides all the essential transformation capabilities we needed.

### Introducing DuckDB

[DuckDB](https://duckdb.org/) is an in-process analytical data management system (OLAP RDBMS). Think of it as SQLite for analytics.

Its key advantages for our use case are:

- **Simplicity:** There's no separate server process to manage; it runs within your Python application or CLI session. Installation is as easy as:
  <!-- svelte-ignore a11y_no_noninteractive_tabindex -->
  ```shell
  pip install duckdb
  ```
- **Speed:** DuckDB is highly optimized for analytical queries and transformations, employing techniques like columnar storage and vectorized query execution.
- **SQL-centric:** It speaks SQL fluently, making it a perfect partner for dbt's SQL-first approach.
- **Versatile Data Ingestion:** DuckDB can directly query data from various file formats, including Parquet, CSV, and JSON, often without needing a separate loading step. For example with Parquet:
  <!-- svelte-ignore a11y_no_noninteractive_tabindex -->

  ```python
  con = duckdb.connect(DUCKDB_DATABASE)
  con.execute("CREATE SCHEMA IF NOT EXISTS public")
  query = f"""
  CREATE TABLE IF NOT EXISTS public.{table_name} AS
  SELECT * FROM read_parquet('{parquet_path}');
  """
  con.execute(query)
  ```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/populate.py" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

### The Combined Architecture

When dbt-core and DuckDB are used together, they form a lightweight yet powerful data transformation engine.

Here's a conceptual flow of how they interact in this project:

<div class="mermaid-container">

```mermaid
graph TD;
    %%{init: {'themeVariables': { 'textColor': '#faf0e6', 'lineColor': '#cccccc', 'fontSize': '14px', 'edgeLabelBackground': '#222', 'clusterBkg': 'transparent', 'clusterBorder': '#cccccc'}}}%%
    subgraph step1Graph [Step 1: Data Ingestion Process]
        A_ingest["Source Data&#10(e.g., WDI CSV, API extracts)"] --> B_ingest["Ingestion Scripts&#10(e.g., Python)"];
        B_ingest -- "(writes to)" --> C_raw_storage["Raw Data Storage&#10(e.g., Local Filesystem,&#10Cloud Bucket - Parquet/CSV)"];
    end

    C_raw_storage -- "(raw data feeds into)" --> D_duckdb_engine["Step 2: DuckDB as Engine&#10(reads raw data, acts as dbt's 'warehouse')"];
    D_duckdb_engine -- "(dbt-core runs SQL/Python transformations)" --> E_duckdb_transformed["Step 3: DuckDB with Transformed Data&#10(stores intermediate & final results)"];
    E_duckdb_transformed --> X_extraction_script["Extraction Script&#10(e.g. Python)"];
    X_extraction_script -- "(writes to)" --> F_processed_storage["Step 4: Processed Data Storage&#10(e.g., Local Filesystem,&#10Cloud Bucket - Parquet,&#10Relational DB like Cloudflare D1)"];
    F_processed_storage --> G_consumption["(consumed by)&#10(Analysis / Visualization / Applications)"];

    classDef mainNode fill:#222,stroke:#cccccc,stroke-width:2px;

    class A_ingest mainNode;
    class B_ingest mainNode;
    class C_raw_storage mainNode;
    class D_duckdb_engine mainNode;
    class E_duckdb_transformed mainNode;
    class F_processed_storage mainNode;
    class G_consumption mainNode;
    class X_extraction_script mainNode;

		style step1Graph fill:transparent,stroke:#faf0e6,color:#faf0e6;


```

</div>

**Explanation of the flow:**

1.  **Data Ingestion:** Raw data is acquired from sources (CSVs, APIs, etc.) and stored locally to be available to DuckDB. In our project we also replicate this data to Cloudflare R2 object store so we have a full record. Parquet is often a good choice for the storage format due to its efficiency.
2.  **DuckDB as the Engine:** DuckDB is configured as the "data warehouse" in dbt's configuration with the database running locally:

<!-- svelte-ignore a11y_no_noninteractive_tabindex -->

```yaml
wdi:
	outputs:
		prod:
			type: duckdb
			path: ../wdi.duckdb
			threads: 4
```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/profiles.yml" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

3.  **dbt Orchestration:** `dbt-core` executes the SQL (or Python) models we've defined. These models perform transformations (cleaning, joining, aggregating) on the data within DuckDB. This is represented by the transition from DuckDB acting as a reader of raw data to DuckDB holding transformed data, orchestrated by dbt. dbt manages the order of execution based on dependencies.
4.  **Output & Consumption:** The transformed data resides within DuckDB. From there, it can be queried directly for analysis, or in our project materialized (written out) to [Parquet stored in Cloudflare R2](https://github.com/nickbrett1/dbt-duc kdb/blob/main/sync_remote_parquet.py) as well as the [relational database Cloudflare D1](https://github.com/nickbrett1/dbt-duckdb/blob/main/update_d1.py).

This architecture allows us to run the entire dbt transformation pipeline locally as part of our [CircleCI CI/CD environment](https://github.com/nickbrett1/dbt-duckdb/blob/main/.circleci/config.yml), using DuckDB as the persistent or ephemeral engine, without needing a dedicated data warehouse service.

### The Results

The accompanying [GitHub project](https://github.com/nickbrett1/dbt-duckdb/tree/main) provides a concrete implementation of this approach using World Development Indicator data. And below are some live visualizations of the transformed data:

<div class="grid grid-cols-1 md:grid-cols-2 gap-6 my-6">

<Card disableHoverGlow={true} >
	<div class="w-full">
		<div class="text-xl font-medium text-gray-300 mt-0 mb-1">Total Countries Tracked</div>
		<div class="text-4xl font-semibold text-white my-0">{totalCountries || 'N/A'}</div>
	</div>
</Card>

<Card disableHoverGlow={true}>
	<div class="w-full">
		<div class="text-xl font-medium text-gray-300 mt-0 mb-1">Total Indicators Tracked</div>
		<div class="text-4xl font-semibold text-white my-0">{totalIndicators || 'N/A'}</div>
	</div>
</Card>

<Card disableHoverGlow={true} class="md:col-span-2">
	<div class="w-full">
		<div class="text-xl font-medium text-gray-300 mt-0 mb-4">
			{dataPointsPerYearChart?.title || 'Data Points Per Year'}
		</div>
		<div class="min-h-[350px]">
			{#if !browser}
				<p class="text-white">Chart will render on client.</p>
			{:else if dataPointsPerYearChart?.series?.[0]?.data?.length > 0}
				<div id={chartElementId}></div>
			{:else if browser && !dataPointsPerYearChart?.series?.[0]?.data?.length > 0 && apexChartInstance}
				<p class="text-white">Chart rendered, but no data points.</p>
			{:else}
				<p class="text-white">No data available for the data points per year chart.</p>
			{/if}
		</div>
	</div>
</Card>

</div>

## Appendix: Additional Considerations

Beyond the core setup, a few additional learnings from the project are worth noting:

### Development Environment: DuckDB vs. Postgres for Concurrency

A practical challenge encountered during development was DuckDB's default behavior with file-based databases: typically, only one process can write to a DuckDB database at a time. In a development environment, you might have multiple tools needing concurrent access – `dbt` running transformations, a VS Code SQL plugin for querying, a database client like Beekeeper Studio for inspection, and the DuckDB CLI itself.

To address this, a local PostgreSQL database was used for development. dbt's [profiles.yml](https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/profiles.yml) file makes it straightforward to define multiple environments. This allowed for easy switching: using Postgres for interactive development where concurrency was beneficial, and DuckDB for "production-like" runs.

### Leveraging `rclone` for Cloud Storage Agnosticity

To manage the movement of data to and from cloud storage (like Cloudflare R2), [rclone ("rsync for cloud storage")](https://rclone.org/) proved to be an invaluable tool.

The benefits of using `rclone` include:

- **Simplified Scripting:** It provides a unified command-line interface for interacting with numerous cloud storage providers (Cloudflare R2, AWS S3, Google Cloud Storage, etc.), simplifying data synchronization scripts.
- **Vendor Independence:** Using `rclone` offers a degree of vendor independence. For our project we chose to use Cloudflare R2 due to its cost-effectiveness, particularly its absence of egress fees, making it attractive for scenarios where data might be pulled for analysis by various tools or services. However if you decide to switch cloud storage providers in the future, the changes required in your data synchronization scripts are minimal, primarily involving `rclone` configuration updates.

[^1]: For example, the [Machine Learning for Flight Delays](https://github.com/nickbrett1/data-science-on-gcp/) project leverages Google Cloud's comprehensive data and machine learning services.
