<script>
	import ProjectLinkButton from '$lib/components/ProjectLinkButton.svelte';
	import TechnologyPills from '$lib/components/TechnologyPills.svelte';

	const projectUrl = "https://github.com/nickbrett1/dbt-duckdb";
	const techs = ["dbt", "DuckDB", "Cloudflare"];
</script>

# Modern Data Transformation without a Data Warehouse: A dbt and DuckDB approach

<TechnologyPills technologies={techs} />
<ProjectLinkButton href={projectUrl} />

## 1. Introduction

Data transformation is a critical step in any data pipeline. For many use cases, cloud data warehouses offer powerful solutions, providing scalability for large datasets and easy access to analytical tools. However, this power often comes with increased complexity, cost, and maintenance overhead. At the other end of the spectrum, simple scripting with file formats like Parquet offers ease of use but can lack the robust structure needed for managing complex transformations as projects evolve.

This article addresses the gap between these two approaches. We present a solution for scenarios where datasets are manageable in size, yet there's a strong need for well-defined, maintainable transformations without incurring the costs and complexities of a full-scale data warehouse. We'll demonstrate how to build such a pipeline locally and deploy with minimal cloud infrastructure by leveraging the combined strengths of dbt-core and DuckDB. This approach offers reduced cloud dependencies for your core transformation engine, lower costs, and a simplified setup and management process. As a practical example, we'll walk through processing World Development Indicator data (https://databank.worldbank.org/source/world-development-indicators), showing code snippets and illustrating how results can be derived directly from our local data stack.

All the code for this project is available on GitHub here: [https://github.com/nickbrett1/dbt-duckdb](https://github.com/nickbrett1/dbt-duckdb)

## 2. The Situation: The Need for Structured Transformation

A common requirement in many data-driven projects is the ability to ingest data from various sources, transform it into a usable format, and prepare it for analysis, reporting, or downstream applications. For this article, we'll focus on a project involving the World Development Indicator (WDI) data from the World Bank. While the example is simplified for clarity, the underlying requirements reflect common real-world data transformation complexities.

Our specific goals for the WDI project were:

1.  **Integrated Data Model:** Our primary data source was the WDI CSV download (from `https://databank.worldbank.org/data/download/WDI_CSV.zip`), but we also needed to enrich this by integrating data from the World Bank's REST API (e.g., `http://api.worldbank.org`). A key requirement was to unify these sources into a consistent data model with standardized naming conventions to simplify downstream analysis.
2.  **Data Reshaping for Analysis:** The raw WDI data often presents historical figures in a wide, column-oriented format (e.g., separate columns for each year). To facilitate easier querying and time-series analysis, we aimed to reshape this into a long, row-oriented format, where each year's data for an indicator and country becomes a distinct row.
3.  **Clear Data Layering:** We emphasized a clear architectural separation between raw, ingested data (staging layer) and the polished, consumer-ready data (marts layer). This separation is crucial for maintainability, allowing us to evolve ingestion or intermediate transformation logic without immediately impacting downstream users.
4.  **Pre-calculated Aggregations:** To optimize query performance and reduce computational load for common analytical questions, we planned to pre-calculate and provide frequently used aggregations.
5.  **Comprehensive Documentation and Lineage:** High-quality documentation was a priority. This included clear definitions for the data interface provided to consumers, as well as transparent lineage explaining how each data point was derived and transformed from its original source.
6.  **Flexible and Cost-Effective Output:** To ensure flexibility for downstream consumers and manage operational costs, our strategy was to primarily deliver final datasets as Parquet files in a cloud object store (Cloudflare R2 in our case, chosen for its cost-effectiveness and lack of egress fees). We also aimed to demonstrate the capability to load this transformed data into a relational database (e.g., Cloudflare's D1 serverless SQLite), offering consumers multiple access patterns.

A crucial overarching constraint was to achieve these goals with a solution that was easy to maintain, could adapt as requirements changed, and avoided the complexity and associated costs of setting up and managing a full-scale cloud data warehouse, which felt like overkill for the scale of this particular task.

## 3. The Complication: Common Hurdles

Achieving the specific goals outlined above efficiently and sustainably, especially while trying to keep infrastructure lean, presents challenges when considering common approaches. Let's compare two ends of the spectrum: simple scripting versus a full cloud data warehouse and supporting tools.

| Feature/Requirement                | Simple Scripting & File-based | Cloud Data Warehouse                      |
| ---------------------------------- | ----------------------------- | ----------------------------------------- |
| **Transformation Structure**       | Ad-hoc, low reusability       | Structured, high reusability              |
| **Maintainability & Evolvability** | Difficult as complexity grows | Good, designed for evolution              |
| **Data Lineage & Docs**            | Manual, often lacking         | Often automated/integrated                |
| **Automated Testing**              | Rudimentary or manual         | Well-supported                            |
| **Analytic Tool Integration**      | Basic (files)                 | Rich (SQL, connectors)                    |
| **Infrastructure Overhead**        | Minimal                       | Managed by provider; setup can be complex |
| **Cost (Compute & Storage)**       | Low                           | Potentially High                          |
| **Scalability (Large Data)**       | Limited                       | Excellent                                 |

As the table illustrates, neither extreme perfectly aligns with our project's needs:

**Challenge 1: The Cloud Data Warehouse Approach - When Comprehensive Becomes Complex**
For projects like ours, opting for a full **Cloud Data Warehouse (CDW)** solution (e.g., Snowflake, Google BigQuery, Amazon Redshift) presents a significant hurdle: the inherent **complexity and overhead of its comprehensive ecosystem**. While CDWs excel at structured transformations, lineage, and scaling for very large datasets—often integrating with a rich array of tools for ingestion, orchestration, and BI—this power comes at a cost. Setting up, configuring, and managing these interconnected services, even with provider-managed infrastructure, introduces a layer of operational complexity and potential expense that can be disproportionate for smaller to medium-scale data transformation tasks like our WDI project. Furthermore, committing to a specific CDW often involves a degree of vendor lock-in, which can have long-term cost and flexibility implications. This was an overarching problem we aimed to avoid.

**Challenge 2: The Simple Scripting Approach - Trading Robustness for Initial Simplicity**
Conversely, while a **Simple Scripting & File-based** approach (e.g., using Python or Shell scripts) offers low initial cost and setup effort, it typically sacrifices long-term **robustness, maintainability, and scalability**. The free-form nature of scripts makes it hard to reason about the transformations occurring, especially as the number of data sources, transformation steps, and interdependencies grows—as was the case with our WDI project's multiple goals. This lack of inherent structure leads to difficulties in testing, debugging, and evolving the pipeline, particularly in a team environment.

Specific pain points frequently encountered with script-based transformation approaches include:

- **Lack of a clear structure & hidden side-effects:** The free-form nature means transformations can be opaque. Defining and understanding dependencies between steps is challenging, and there's no inherent guarantee against unintended side-effects within the transformation logic.
- **Testing difficulties:** The lack of a defined contract for inputs and outputs makes it cumbersome to isolate and test individual transformations. Swapping out data sources or mocking dependencies for testing purposes can be particularly difficult.
- **Poor data lineage:** It's hard to trace how data is transformed from its source to the final output, making debugging and impact analysis difficult.
- **Out-of-sync documentation:** Documentation, if it exists, often lags behind code changes.
- **Error-prone refactoring:** Modifying existing transformations or adding new ones can easily introduce errors due to hidden dependencies or lack of clarity.

## 4. The Question: Seeking a Better Way

Given these challenges, the central question becomes:

How can we implement a data transformation workflow that is:

- **Well-structured and easy to evolve**, addressing the maintainability issues inherent in ad-hoc scripting?
- **Low-cost and simple to deploy and manage**, mitigating infrastructure overhead and avoiding unnecessary cloud data warehouse lock-in for the core transformation engine?
- **Capable of efficiently handling typical analytical transformations** like joins, aggregations, and data cleaning?

## 5. The Answer: dbt-core Meets DuckDB

The solution lies in the powerful combination of **dbt-core** and **DuckDB**.

**Part 1: Introducing dbt (Data Build Tool)**

dbt is an open-source command-line tool that empowers data analysts and engineers to transform data within their data store more effectively. It brings software engineering best practices to the analytics workflow.

Its core capabilities include:

- **SQL-centric (and Python):** Define transformations primarily using SQL (with Python model support in newer versions), a language familiar to most data practitioners.
- **Dependency Management:** dbt automatically understands and manages the dependencies between your transformation steps (called "models").
- **Automated Testing:** Implement data quality tests to ensure the integrity of your transformations and output data.
- **Documentation & Lineage:** Automatically generate comprehensive documentation and a visual lineage graph, showing how data flows through your pipeline.
- **Version Control:** Treat your transformation logic as code, enabling version control with tools like Git.

For this article, we focus on `dbt-core`, the open-source Python package that you can run anywhere. There's also `dbt Cloud`, a managed cloud-based solution offering additional features like a scheduler and UI, but `dbt-core` provides all the essential transformation capabilities.

**Part 2: Introducing DuckDB**

DuckDB is an in-process analytical data management system (OLAP RDBMS). Think of it as SQLite for analytics.

Its key advantages for our use case are:

- **Simplicity:** Installation is as easy as `pip install duckdb`. There's no separate server process to manage; it runs within your Python application or CLI session.
- **Speed:** DuckDB is highly optimized for analytical queries and transformations, employing techniques like columnar storage and vectorized query execution.
- **SQL-centric:** It speaks SQL fluently, making it a perfect partner for dbt's SQL-first approach.
- **Versatile Data Ingestion:** DuckDB can directly query data from various file formats, including Parquet, CSV, and JSON, often without needing a separate loading step.

**Part 3: The Combined Architecture**

When dbt-core and DuckDB are used together, they form a lightweight yet powerful data transformation engine.

Here's a conceptual flow of how they interact:

```mermaid
graph TD;
    subgraph "Step 1: Data Ingestion Process"
        A["Source Data
        (e.g., WDI CSV, API extracts)"] --> B["Ingestion Scripts
        (e.g., Python)"];
        B -- "(writes to)" --> C["Raw Data Storage
        (e.g., Local Filesystem,
        Cloud Bucket - Parquet/CSV)"];
    end

    C -- "(raw data feeds into)" --> D["Step 2: DuckDB as Engine
    (reads raw data, acts as dbt's 'warehouse')"];
    D -- "(dbt-core runs SQL/Python transformations)" --> E["Step 3: DuckDB with Transformed Data
    (stores intermediate & final results)"];
    E --> F["Step 4: Processed Data Storage
    (e.g., Local Filesystem,
    Cloud Bucket - Parquet,
    Relational DB like Cloudflare D1)"];
    F --> G["(consumed by)
    Analysis / Visualization / Applications"];
```

**Explanation of the flow:**

1.  **Data Ingestion:** Raw data is acquired from sources (CSVs, APIs, etc.) and typically landed in a staging area. This could be local files or an object store like Cloudflare R2. Parquet is often a good choice for the storage format due to its efficiency.
2.  **DuckDB as the Engine:** DuckDB is configured as the "data warehouse" in dbt's configuration. It runs locally and reads the raw data directly from the data storage location.
3.  **dbt Orchestration:** `dbt-core` executes the SQL (or Python) models you've defined. These models perform transformations (cleaning, joining, aggregating) on the data within DuckDB. This is represented by the transition from DuckDB acting as a reader of raw data to DuckDB holding transformed data, orchestrated by dbt. dbt manages the order of execution based on dependencies.
4.  **Output & Consumption:** The transformed data resides within DuckDB. From there, it can be queried directly for analysis, or materialized (written out) to Processed Data Storage for use by other tools or applications.

This architecture allows you to run your entire dbt transformation pipeline locally or in a simple CI/CD environment, using DuckDB as the ephemeral or persistent engine, without needing a dedicated data warehouse service.

**Part 4: Showcasing the Results (from the `dbt-duckdb` project)**

The accompanying GitHub project provides a concrete implementation of this approach using World Development Indicator data. And below are some example visualizations of the transformed data:

- **Embedded Visualizations/Data (Illustrative):**
  - _(Placeholder: Imagine a line graph generated using a Python library like Matplotlib or Seaborn, plotting a key WDI indicator (e.g., GDP per capita) over time for selected countries. This graph would be generated by querying the final transformed data from the DuckDB instance.)_
  - _(Placeholder: Key summary statistics derived from the transformed data, such as the total number of countries processed, the range of years covered, and the count of unique indicators, could be presented here. For example: "Processed data for 217 countries across 50+ indicators from 1960 to 2022.")_

## 6. Appendix: Additional Considerations & Learnings

Beyond the core setup, a few additional considerations and learnings from the `dbt-duckdb` project are worth noting:

**A1: Development Environment - DuckDB vs. Postgres for Concurrency**

A practical challenge encountered during development was DuckDB's default behavior with file-based databases: typically, only one process can write to a database file at a time. In a development environment, you might have multiple tools needing concurrent access – `dbt` running transformations, a VS Code SQL plugin for querying, a database client like Beekeeper Studio for inspection, and the DuckDB CLI itself.

To address this, a local PostgreSQL database was used for development. dbt's `profiles.yml` makes it straightforward to define multiple target profiles. This allowed for easy switching: using Postgres for interactive development where concurrency was beneficial, and DuckDB for "production-like" runs.

**A2: Leveraging `rclone` for Cloud Storage Agnosticity**

To manage the movement of data to and from cloud storage (like Cloudflare R2 in the example project), `rclone` ("rsync for cloud storage") proved to be an invaluable tool.

The benefits of using `rclone` include:

- **Simplified Scripting:** It provides a unified command-line interface for interacting with numerous cloud storage providers (Cloudflare R2, AWS S3, Google Cloud Storage, etc.), simplifying data synchronization scripts.
- **Vendor Independence:** Using `rclone` offers a degree of vendor independence. For our project we chose to use Cloudflare R2 due to its cost-effectiveness, particularly its absence of egress fees, making it attractive for scenarios where data might be pulled for analysis by various tools or services. However if you decide to switch cloud storage providers in the future, the changes required in your data synchronization scripts are minimal, primarily involving `rclone` configuration updates.
