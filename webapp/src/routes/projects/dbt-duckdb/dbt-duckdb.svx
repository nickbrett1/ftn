<script>
	import ProjectLinkButton from '$lib/components/ProjectLinkButton.svelte';
	import TechnologyPills from '$lib/components/TechnologyPills.svelte';
	import Img from '@zerodevx/svelte-img';
	import twoButtonsMeme from "$lib/images/twobuttonsmeme.jpg?as=run";

	const projectUrl = "https://github.com/nickbrett1/dbt-duckdb";
	const techs = ["dbt", "DuckDB", "Cloudflare"];

	import { GithubBrands } from 'svelte-awesome-icons';
</script>

<style>
	.responsive-text-table :global(pre.shiki) {
		white-space: pre-wrap; /* Preserves whitespace, but wraps lines */
		overflow-wrap: break-word; /* Breaks long words to prevent overflow */
		word-break: break-all; /* More aggressive word breaking if needed */
		overflow-x: hidden; /* Explicitly hide horizontal scrollbar if wrapping is effective */
	}
	.mermaid-container {
		display: flex;
		justify-content: center;
	}
</style>

<h1 class="!mb-0 no-underline">Modern ETL without a Data Warehouse</h1>
<p class="text-xl text-gray-300 !mt-2 !mb-6">A dbt and DuckDB approach</p>

<ProjectLinkButton href={projectUrl} />

## Introduction

Data transformation is a critical step in any data pipeline. For many use cases, a full-scale cloud data warehouse offers powerful solutions, providing scalability for large datasets and easy access to analytical tools. However, this power often comes with increased complexity, cost, and maintenance overhead. At the other end of the spectrum, a few simple scripts working on a data dump offers an easy starting point but lacks the robust structure needed for managing complex transformations as projects evolve.

<!-- Use https://imgflip.com/ for memes -->
<Img src={twoButtonsMeme} alt="Meme illustrating the choice between simple scripts and complex cloud data warehouses" class="mx-auto my-4 max-w-md" />

This article describes a middle ground between these two approaches, aimed at scenarios where datasets are manageable in size, somewhat independent, for example for driving trading decisions, and yet there's a strong need for well-defined, maintainable transformations without incurring the costs and complexities of a full-scale data warehouse.

We'll demonstrate how to build such a pipeline and deploy with minimal infrastructure by leveraging the combined strengths of [dbt-core](https://github.com/dbt-labs/dbt-core) and [DuckDB](https://duckdb.org/). This approach offers reduced cloud dependencies for the core transformation engine, lower costs, and a simplified setup and management process. As a practical example, we'll use the [World Bank World Development Indicators](https://databank.worldbank.org/source/world-development-indicators), showing code snippets and illustrating how robust results can be derived from a relatively simple local data stack.

## Goals

Our specific goals for this project were:

1.  **Integrated Data Model:** Our primary data source was the [WDI CSV download](https://databank.worldbank.org/data/download/WDI_CSV.zip), but we also needed to enrich this by integrating data from the [World Bank's REST API](https://datahelpdesk.worldbank.org/knowledgebase/articles/898590-api-overview). A key requirement was to unify these sources into a consistent data model with standardized naming conventions.
2.  **Data Reshaping for Analysis:** The raw WDI data often presents historical figures in a wide, column-oriented format. To facilitate easier querying and time-series analysis, we aimed to reshape this into a long, row-oriented format.

For example:

<div class="responsive-text-table">

<!-- svelte-ignore a11y-no-noninteractive-tabindex -->

```text
// Wide Format (Column-Oriented)
// Each row represents one country & indicator, with years as columns.

Country Name | Indicator Name | 1960 | 1961 | ...
-------------|----------------|------|------|----
Aruba        | GDP per capita | 100  | 102  | ...
Afghanistan  | Population     | 5000 | 5050 | ...
```

</div>
<div class="responsive-text-table">
<!-- svelte-ignore a11y-no-noninteractive-tabindex -->

```text
// Long Format (Row-Oriented)
// Each row represents one country, indicator, and year observation.

Country Name | Indicator Name | Year | Value
-------------|----------------|------|-------
Aruba        | GDP per capita | 1960 | 100
Aruba        | GDP per capita | 1961 | 102
Afghanistan  | Population     | 1960 | 5000
Afghanistan  | Population     | 1961 | 5050
...          | ...            | ...  | ...
```

</div>

3.  **Clear Data Layering:** We emphasized a clear architectural separation between raw data, an intermediate staging layer, and the final marts layer.

        - **Raw Data Layer:** This is the initial landing zone for data as it's ingested from various sources (e.g., CSV files, API extracts). The data here is typically in its original, unaltered format, reflecting the structure and content provided by the source systems.
        - **Staging Layer:** This layer takes the raw, ingested data and applies initial transformations, such as standardizing column names (e.g., converting "Country Name" to `country_name`), casting data types, and basic cleaning. This ensures a consistent foundation for subsequent transformations.
        - **Marts Layer:** This layer builds upon the staging layer to create polished, consumer-ready datasets, often aggregated or reshaped for specific analytical purposes.

        This separation is crucial for maintainability, allowing us to evolve ingestion or intermediate transformation logic without immediately impacting downstream users. The diagram below illustrates this flow:

<!-- svelte-ignore a11y-no-noninteractive-tabindex -->
<div class="mermaid-container">

```mermaid
graph TD;
		A["Raw Data Layer&#10(Original Format)"] --> B["Staging Layer&#10(Standardized & Cleaned)"];
		B --> C["Marts Layer&#10(Aggregated & Reshaped)"];
		C --> D["Analytics & Applications"];

		click A "https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/staging/_wdi_sources.yml" "View Raw Data Layer dbt sources" _self
		click B "https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/staging/_wdi_raw_models.yml" "View Staging Layer dbt models" _self
		click C "https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/marts/_wdi_marts.yml" "View Marts Layer dbt models" _self

		classDef default fill:#222,stroke:#cccccc,stroke-width:2px,color:#faf0e6;
		linkStyle default stroke:#cccccc,stroke-width:2px;

```

</div>
        <!-- svelte-ignore a11y-no-noninteractive-tabindex -->

4.  **Pre-calculated Aggregations:** To optimize query performance and reduce computational load for common analytical questions, we wanted to be able to pre-calculate frequently used aggregations.
5.  **Comprehensive Documentation and Lineage:** We wanted a way to provide documentation on the transformation, a clean way to provide definitions for the data interface, and transparent lineage explaining how each data point was derived from its original source.
6.  **Flexible and Cost-Effective Output:** To ensure flexibility for any downstream consumers, our strategy was to primarily deliver final datasets as Parquet files in a cloud object store ([Cloudflare R2](https://www.cloudflare.com/developer-platform/r2/) in our case, chosen for its cost-effectiveness and lack of egress fees). We also aimed to demonstrate the capability to easily load this transformed data into a relational database (e.g., [Cloudflare's D1](https://www.cloudflare.com/developer-platform/d1/) serverless SQLite).
7.  **Low-Complexity and Cost-Effective Solution:** We also wanted a solution that was easy to maintain and was relatively low cost given the scale of the project.

## Why Standard Solutions Can Fall Short

Let's compare the two standard solutions: simple scripting versus a full cloud data warehouse and supporting tools.

| Goal                                         | Simple Scripting & File-based                                                                   | Cloud Data Warehouse                                                                                                                |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| **Integrated Data Model**                    | ðŸ”´ Difficult to manage consistency as complexity grows. Ad-hoc.                                 | ðŸŸ¢ Strong. Designed for structured, relational data models.                                                                         |
| **Data Reshaping for Analysis**              | ðŸŸ¡ Possible, but complex reshaping (e.g., wide to long) can become unwieldy and error-prone.    | ðŸŸ¢ Excellent. SQL provides powerful reshaping capabilities.                                                                         |
| **Clear Data Layering**                      | ðŸ”´ Lacks inherent structure for layering; becomes a tangled web of scripts.                     | ðŸŸ¢ Good. Supports logical separation (e.g., staging, marts).                                                                        |
| **Pre-calculated Aggregations**              | ðŸŸ¡ Can be done, but managing dependencies and updates is manual and complex.                    | ðŸŸ¢ Strong. Materialized views or summary tables are common.                                                                         |
| **Comprehensive Documentation & Lineage**    | ðŸ”´ Almost entirely manual and often neglected. Difficult to trace data flow.                    | ðŸŸ¢ Good. Many tools offer automated documentation and lineage.                                                                      |
| **Flexible & Cost-Effective Output**         | ðŸŸ¡ Flexible in output formats (files). Cost-effective for simple outputs.                       | ðŸŸ¡ Flexible (SQL access, connectors), but storage/egress can be costly.                                                             |
| **Low-Complexity & Cost-Effective Solution** | ðŸŸ¡ Low initial complexity & cost. Becomes very complex and costly to maintain as project grows. | ðŸ”´ High initial setup complexity & potentially high ongoing costs. Managed services reduce some operational burden but add to cost. |

As the table illustrates, neither extreme perfectly aligns with our project's needs:

### The Cloud Data Warehouse Approach: When Comprehensive Becomes Complex

For projects like ours, opting for a full **Cloud Data Warehouse (CDW)** solution (e.g., Snowflake, Google BigQuery, Amazon Redshift) presents a significant hurdle: the inherent **complexity and overhead of its comprehensive ecosystem**. While CDWs excel at structured transformations, lineage, and scaling for very large datasetsâ€”often integrating with a rich array of tools for ingestion, orchestration, and BIâ€”this power comes at a cost. Setting up, configuring, and managing these interconnected services, even with provider-managed infrastructure, introduces a layer of operational complexity and potential expense that can be disproportionate for smaller to medium-scale data transformation tasks like our WDI project. Furthermore, committing to a specific CDW often involves a degree of vendor lock-in, which can have long-term cost and flexibility implications. This was an overarching problem we aimed to avoid.

### The Simple Scripting Approach: Trading Robustness for Initial Simplicity

Conversely, while a **Simple Scripting & File-based** approach (e.g., using Python or Shell scripts) offers low initial cost and setup effort, it typically sacrifices long-term **robustness, maintainability, and scalability**. The free-form nature of scripts makes it hard to reason about the transformations occurring, especially as the number of data sources, transformation steps, and interdependencies growsâ€”as was the case with our project's multiple goals. This lack of inherent structure leads to difficulties in testing, debugging, and evolving the pipeline, particularly in a team environment.

## Seeking a Better Way

Given these challenges, the central question becomes:

How can we implement a data transformation workflow that is:

1. Well-structured and easy to evolve?
2. Low-cost and simple to deploy and manage?
3. Capable of efficiently handling typical analytical transformations?

## The Solution: dbt-core Meets DuckDB

The solution we use lies in the powerful combination of **dbt-core** and **DuckDB**.

### Introducing dbt (Data Build Tool)

dbt is an open-source command-line tool that empowers data analysts and engineers to transform data within their data store more effectively. It brings software engineering best practices to the analytics workflow.

Its core capabilities include:

- **SQL-centric (and Python):** Define transformations primarily using SQL (with Python model support in newer versions), a language familiar to most data practitioners.
  For example, one aggregation transformation from our project looks like this:
    <!-- svelte-ignore a11y-no-noninteractive-tabindex -->

  ```sql
  select year, count(value) as data_points_count
  from {{ ref('fct_wdi_history') }}
  where value is not null
  group by year
  ```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/marts/agg_wdi_data_points_by_year.sql" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

- **Dependency Management:** dbt automatically understands and manages the dependencies between transformation steps (called "models").
- **Automated Testing:** Implement data quality tests to ensure the integrity of your transformations and output data.
  For example, we can test for nulls by providing yaml configuration to dbt:
  <!-- svelte-ignore a11y-no-noninteractive-tabindex -->

  ```yaml
  - name: indicator_code
  description: "Indicator code."
  tests:
  	- not_null
  ```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/wdi/models/marts/_wdi_marts.yml" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

- **Documentation & Lineage:** Automatically generate comprehensive documentation and a visual lineage graph, showing how data flows through your pipeline. See [here](./dbt-duckdb/docs) for the generated documentation for the project.
- **Version Control:** Treats your transformation logic as code, enabling version control.

For this project, we used [dbt-core](https://github.com/dbt-labs/dbt-core), the open-source Python package that you can run anywhere. There's also [dbt Cloud](https://www.getdbt.com/), a managed cloud-based solution offering additional features like a scheduler and UI, but `dbt-core` provides all the essential transformation capabilities we needed.

### Introducing DuckDB

[DuckDB](https://duckdb.org/) is an in-process analytical data management system (OLAP RDBMS). Think of it as SQLite for analytics.

Its key advantages for our use case are:

- **Simplicity:** There's no separate server process to manage; it runs within your Python application or CLI session. Installation is as easy as:
  <!-- svelte-ignore a11y-no-noninteractive-tabindex -->
  ```shell
  pip install duckdb
  ```
- **Speed:** DuckDB is highly optimized for analytical queries and transformations, employing techniques like columnar storage and vectorized query execution.
- **SQL-centric:** It speaks SQL fluently, making it a perfect partner for dbt's SQL-first approach.
- **Versatile Data Ingestion:** DuckDB can directly query data from various file formats, including Parquet, CSV, and JSON, often without needing a separate loading step. For example with Parquet:
  <!-- svelte-ignore a11y-no-noninteractive-tabindex -->

  ```python
  con = duckdb.connect(DUCKDB_DATABASE)
  con.execute("CREATE SCHEMA IF NOT EXISTS public")
  query = f"""
  CREATE TABLE IF NOT EXISTS public.{table_name} AS
  SELECT * FROM read_parquet('{parquet_path}');
  """
  con.execute(query)
  ```

  <a href="https://github.com/nickbrett1/dbt-duckdb/blob/main/populate.py" target="_blank" rel="noopener noreferrer" title="View on GitHub" style="display: inline-flex; align-items: center; gap: 0.3em; line-height: 1;">
    <GithubBrands size="1.2em" color="white" /> View on GitHub
  </a>

### The Combined Architecture

When dbt-core and DuckDB are used together, they form a lightweight yet powerful data transformation engine.

Here's a conceptual flow of how they interact in this project:

<div class="mermaid-container">

```mermaid
graph TD;
    %%{init: {'themeVariables': { 'textColor': '#faf0e6', 'lineColor': '#cccccc', 'fontSize': '14px', 'edgeLabelBackground': '#222', 'clusterBkg': 'transparent', 'clusterBorder': '#cccccc'}}}%%
    subgraph step1Graph [Step 1: Data Ingestion Process]
        A_ingest["Source Data&#10(e.g., WDI CSV, API extracts)"] --> B_ingest["Ingestion Scripts&#10(e.g., Python)"];
        B_ingest -- "(writes to)" --> C_raw_storage["Raw Data Storage&#10(e.g., Local Filesystem,&#10Cloud Bucket - Parquet/CSV)"];
    end

    C_raw_storage -- "(raw data feeds into)" --> D_duckdb_engine["Step 2: DuckDB as Engine&#10(reads raw data, acts as dbt's 'warehouse')"];
    D_duckdb_engine -- "(dbt-core runs SQL/Python transformations)" --> E_duckdb_transformed["Step 3: DuckDB with Transformed Data&#10(stores intermediate & final results)"];
    E_duckdb_transformed --> F_processed_storage["Step 4: Processed Data Storage&#10(e.g., Local Filesystem,&#10Cloud Bucket - Parquet,&#10Relational DB like Cloudflare D1)"];
    F_processed_storage --> G_consumption["(consumed by)&#10(Analysis / Visualization / Applications)"];

    classDef mainNode fill:#222,stroke:#cccccc,stroke-width:2px;

    class A_ingest mainNode;
    class B_ingest mainNode;
    class C_raw_storage mainNode;
    class D_duckdb_engine mainNode;
    class E_duckdb_transformed mainNode;
    class F_processed_storage mainNode;
    class G_consumption mainNode;

		style step1Graph fill:transparent,stroke:#faf0e6,color:#faf0e6;


```

</div>

**Explanation of the flow:**

1.  **Data Ingestion:** Raw data is acquired from sources (CSVs, APIs, etc.) and typically landed in a staging area. This could be local files or an object store like Cloudflare R2. Parquet is often a good choice for the storage format due to its efficiency.
2.  **DuckDB as the Engine:** DuckDB is configured as the "data warehouse" in dbt's configuration. It runs locally and reads the raw data directly from the data storage location.
3.  **dbt Orchestration:** `dbt-core` executes the SQL (or Python) models you've defined. These models perform transformations (cleaning, joining, aggregating) on the data within DuckDB. This is represented by the transition from DuckDB acting as a reader of raw data to DuckDB holding transformed data, orchestrated by dbt. dbt manages the order of execution based on dependencies.
4.  **Output & Consumption:** The transformed data resides within DuckDB. From there, it can be queried directly for analysis, or materialized (written out) to Processed Data Storage for use by other tools or applications.

This architecture allows you to run your entire dbt transformation pipeline locally or in a simple CI/CD environment, using DuckDB as the ephemeral or persistent engine, without needing a dedicated data warehouse service.

### Showcasing the Results

The accompanying GitHub project provides a concrete implementation of this approach using World Development Indicator data. And below are some example visualizations of the transformed data:

- **Embedded Visualizations/Data (Illustrative):**
  - _(Placeholder: Imagine a line graph generated using a Python library like Matplotlib or Seaborn, plotting a key WDI indicator (e.g., GDP per capita) over time for selected countries. This graph would be generated by querying the final transformed data from the DuckDB instance.)_
  - _(Placeholder: Key summary statistics derived from the transformed data, such as the total number of countries processed, the range of years covered, and the count of unique indicators, could be presented here. For example: "Processed data for 217 countries across 50+ indicators from 1960 to 2022.")_

## Appendix: Additional Considerations

Beyond the core setup, a few additional considerations and learnings from the `dbt-duckdb` project are worth noting:

### Development Environment: DuckDB vs. Postgres for Concurrency

A practical challenge encountered during development was DuckDB's default behavior with file-based databases: typically, only one process can write to a database file at a time. In a development environment, you might have multiple tools needing concurrent access â€“ `dbt` running transformations, a VS Code SQL plugin for querying, a database client like Beekeeper Studio for inspection, and the DuckDB CLI itself.

To address this, a local PostgreSQL database was used for development. dbt's `profiles.yml` makes it straightforward to define multiple target profiles. This allowed for easy switching: using Postgres for interactive development where concurrency was beneficial, and DuckDB for "production-like" runs.

### Leveraging `rclone` for Cloud Storage Agnosticity

To manage the movement of data to and from cloud storage (like Cloudflare R2 in the example project), `rclone` ("rsync for cloud storage") proved to be an invaluable tool.

The benefits of using `rclone` include:

- **Simplified Scripting:** It provides a unified command-line interface for interacting with numerous cloud storage providers (Cloudflare R2, AWS S3, Google Cloud Storage, etc.), simplifying data synchronization scripts.
- **Vendor Independence:** Using `rclone` offers a degree of vendor independence. For our project we chose to use Cloudflare R2 due to its cost-effectiveness, particularly its absence of egress fees, making it attractive for scenarios where data might be pulled for analysis by various tools or services. However if you decide to switch cloud storage providers in the future, the changes required in your data synchronization scripts are minimal, primarily involving `rclone` configuration updates.
